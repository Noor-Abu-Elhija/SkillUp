{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9f3310f-c1dc-462e-b3b5-c4530aec350c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.types import *\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
    "from pyspark.sql.types import StringType, ArrayType,BooleanType\n",
    "from langdetect import detect, DetectorFactory\n",
    "import tensorflow_hub as hub\n",
    "from sklearn.cluster import DBSCAN\n",
    "import google.generativeai as genai\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from scipy.cluster.hierarchy import linkage, fcluster, dendrogram\n",
    "import tensorflow\n",
    "from sklearn.metrics import silhouette_score\n",
    "from pyspark.sql.functions import col, explode, count, desc, collect_list, udf, concat_ws, row_number\n",
    "from pyspark.sql.window import Window\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import google.generativeai as genai\n",
    "import time\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "from sklearn.metrics import calinski_harabasz_score\n",
    "from sklearn.metrics import silhouette_samples\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import json\n",
    "import ast\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94bb77dc-46f4-4bc2-9343-940019c0ae99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**1. Prepare the Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88b9db3a-af65-4ec3-9f18-bd9297b2548f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pandas_df = pd.read_parquet(\"/Workspace/Users/noor.abu@campus.technion.ac.il/data/glassdoor_jobs.parquet\")\n",
    "pandas_df2= pd.read_parquet(\"/Workspace/Users/noor.abu@campus.technion.ac.il/data/glassdoor_jobs (1).parquet\")\n",
    "df = spark.createDataFrame(pandas_df)\n",
    "df2 = spark.createDataFrame(pandas_df2)\n",
    "df = df.union(df2)\n",
    "print(df.count())\n",
    "df = df.drop_duplicates()\n",
    "print(df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3bac5459-b17d-4c5b-8c4f-b370b693b4ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "state_dict = {\n",
    "  \"AL\": \"Alabama\", \"AK\": \"Alaska\", \"AZ\": \"Arizona\", \"AR\": \"Arkansas\",\n",
    "    \"CA\": \"California\", \"CO\": \"Colorado\", \"CT\": \"Connecticut\", \"DE\": \"Delaware\",\n",
    "    \"FL\": \"Florida\", \"GA\": \"Georgia\", \"HI\": \"Hawaii\", \"ID\": \"Idaho\",\n",
    "    \"IL\": \"Illinois\", \"IN\": \"Indiana\", \"IA\": \"Iowa\", \"KS\": \"Kansas\",\n",
    "    \"KY\": \"Kentucky\", \"LA\": \"Louisiana\", \"ME\": \"Maine\", \"MD\": \"Maryland\",\n",
    "    \"MA\": \"Massachusetts\", \"MI\": \"Michigan\", \"MN\": \"Minnesota\", \"MS\": \"Mississippi\",\n",
    "    \"MO\": \"Missouri\", \"MT\": \"Montana\", \"NE\": \"Nebraska\", \"NV\": \"Nevada\",\n",
    "    \"NH\": \"New Hampshire\", \"NJ\": \"New Jersey\", \"NM\": \"New Mexico\", \"NY\": \"New York\",\n",
    "    \"NC\": \"North Carolina\", \"ND\": \"North Dakota\", \"OH\": \"Ohio\", \"OK\": \"Oklahoma\",\n",
    "    \"OR\": \"Oregon\", \"PA\": \"Pennsylvania\", \"RI\": \"Rhode Island\", \"SC\": \"South Carolina\",\n",
    "    \"SD\": \"South Dakota\", \"TN\": \"Tennessee\", \"TX\": \"Texas\", \"UT\": \"Utah\",\n",
    "    \"VT\": \"Vermont\", \"VA\": \"Virginia\", \"WA\": \"Washington\", \"WV\": \"West Virginia\",\n",
    "    \"WI\": \"Wisconsin\", \"WY\": \"Wyoming\",\n",
    "    \"Minnesota\": \"Minnesota\",\n",
    "    \"DC\": \"District of Columbia\",\n",
    "    \"Georgia\": \"Georgia\",\n",
    "    \"Oahu Island\": \"Hawaii\",\n",
    "    \"Utah\": \"Utah\",\n",
    "    \"PR\": \"Puerto Rico\",\n",
    "    \"Dimmit\": \"Texas\",\n",
    "    \"Missouri\": \"Missouri\",\n",
    "    \"Prince George's\": \"Maryland\",\n",
    "    \"Island of Hawai‘i\": \"Hawaii\",\n",
    "    \"Indiana\": \"Indiana\",\n",
    "    \"Tennessee\": \"Tennessee\",\n",
    "    \"Wisconsin\": \"Wisconsin\",\n",
    "    \"South Carolina\": \"South Carolina\",\n",
    "    \"Virginia\": \"Virginia\",\n",
    "    \"Kentucky\": \"Kentucky\",\n",
    "    \"Harris\": \"Texas\",\n",
    "    \"Nebraska\": \"Nebraska\",\n",
    "    \"Oregon\": \"Oregon\",\n",
    "    \"Mississippi\": \"Mississippi\",\n",
    "    \"Ohio\": \"Ohio\",\n",
    "    \"Illinois\": \"Illinois\",\n",
    "    \"Texas\": \"Texas\",\n",
    "    \"Arizona\": \"Arizona\",\n",
    "    \"Nevada\": \"Nevada\",\n",
    "    \"Mecklenburg\": \"North Carolina\",\n",
    "    \"Kansas\": \"Kansas\",\n",
    "    \"North Carolina\": \"North Carolina\",\n",
    "    \"Washington State\": \"Washington\",\n",
    "    \"New Jersey\": \"New Jersey\",\n",
    "    \"Pennsylvania\": \"Pennsylvania\",\n",
    "    \"Henderson\": \"Nevada\",\n",
    "    \"Arkansas\": \"Arkansas\",\n",
    "    \"Connecticut\": \"Connecticut\",\n",
    "    \"Alaska\": \"Alaska\",\n",
    "    \"Long Island-Queens\": \"New York\",\n",
    "    \"Maine\": \"Maine\",\n",
    "    \"Remote\":\"Remote\",\n",
    "    \"United States\": \"United States\",\n",
    "    \"Iowa\": \"Iowa\",\n",
    "    \"New York State\": \"New York\",\n",
    "    \"Puerto Rico\": \"Puerto Rico\",\n",
    "    \"Michigan\": \"Michigan\",\n",
    "    \"Montana\": \"Montana\",\n",
    "    \"Colorado\": \"Colorado\",\n",
    "    \"Minneapolis-Saint Paul\": \"Minnesota\",\n",
    "    \"Dallas-Fort Worth\": \"Texas\",\n",
    "    \"Alabama\": \"Alabama\",\n",
    "    \"New Mexico\": \"New Mexico\",\n",
    "    \"Redstone Arsenal\": \"Alabama\",\n",
    "    \"Bell\": \"California\",\n",
    "    \"Cuyahoga\": \"Ohio\",\n",
    "    \"Oakland\": \"California\",\n",
    "    \"California\": \"California\",\n",
    "    \"GU\": \"Guam\", \n",
    "      \"Bergen\": \"New Jersey\",  \n",
    "    \"Maryland\": \"Maryland\",\n",
    "    \"Massachusetts\": \"Massachusetts\",\n",
    "    \"Florida\": \"Florida\",\n",
    "    \"St Louis Park\": \"Minnesota\",\n",
    "    \"California\": \"California\",\n",
    "    \"Ford Island\": \"Hawaii\",\n",
    "}\n",
    "df_updated = df.withColumn(\n",
    "    \"Location\",\n",
    "    when(col(\"Location\").contains(\",\"), trim(split(col(\"Location\"), \",\")[1]))\n",
    "    .otherwise(col(\"Location\"))\n",
    ")\n",
    "def map_state(location):\n",
    "    return state_dict.get(location, None)\n",
    "\n",
    "state_udf = udf(map_state, StringType())\n",
    "\n",
    "df_updated = df_updated.withColumn(\"Full State Name\", state_udf(df_updated[\"Location\"]))\n",
    "df_updated = df_updated.na.drop(subset=[\"Full State Name\"])\n",
    "df_updated.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3364bd45-498f-43a6-b9a6-4e0bcd13d601",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**2. Cleaning the Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3710691d-d5f3-46ac-8589-968111219336",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_updated = df_updated.withColumn(\"job_title_lower\", lower(col(\"Job Title\")))\n",
    "df_updated = df_updated.withColumn(\"job_title_cleaned\", regexp_replace(col(\"job_title_lower\"), \"[^a-zA-Z\\s]\", \" \"))\n",
    "custom_stop_words = [\"per\", \"hr\", \"hour\", \"full\", \"time\", \"annual\", \"salary\", \"k\", \"fulltime\",\"days\",\"hours\",\"hrs\",\"day\",\"parttime\",\"part\",\"years\",\"year\",\"am\",\"pm\",\"Part-time\"]\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"job_title_cleaned\", outputCol=\"job_title_tokens\")\n",
    "df_updated = tokenizer.transform(df_updated)\n",
    "\n",
    "stop_words_remover = StopWordsRemover(inputCol=\"job_title_tokens\", outputCol=\"cleaned_tokens\")\n",
    "stop_words_remover.setStopWords(stop_words_remover.getStopWords() + custom_stop_words)  \n",
    "df_updated = stop_words_remover.transform(df_updated)\n",
    "\n",
    "def tokens_to_string(tokens):\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "combine_udf = udf(tokens_to_string, StringType())\n",
    "df_updated = df_updated.withColumn(\"cleaned_job_title\", combine_udf(col(\"cleaned_tokens\"))).select(\"Job Title\", \"Location\", \"Description\", \"Full State Name\", \"cleaned_job_title\")\n",
    "def detect_language(text):\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except:\n",
    "        return \"unknown\" \n",
    "\n",
    "lang_udf = udf(detect_language, StringType())\n",
    "\n",
    "\n",
    "df_updated = df_updated.withColumn(\"language\", lang_udf(col(\"Description\")))\n",
    "df_updated = df_updated.filter(col(\"language\") == \"en\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "83101ae5-f6fa-49bc-88a7-39f045cdf518",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**3. Creating Clusters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c93bea6a-46f3-4286-9d52-6e4da092effb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@pandas_udf(ArrayType(FloatType()))\n",
    "def generate_embeddings(job_titles: pd.Series) -> pd.Series:\n",
    "    model = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "    embeddings = model(job_titles.tolist()).numpy()\n",
    "    return pd.Series(embeddings.tolist())\n",
    "\n",
    "df_with_embeddings = df_updated.withColumn(\"embeddings\", generate_embeddings(df_updated[\"cleaned_job_title\"]))\n",
    "\n",
    "pandas_df = df_with_embeddings.select(\"Job Title\",\"Full State Name\",\"Description\",\"cleaned_job_title\", \"embeddings\").toPandas()\n",
    "embeddings_np = np.array(pandas_df[\"embeddings\"].tolist())\n",
    "linkage_matrix = linkage(embeddings_np, method='ward')\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "dendrogram(linkage_matrix, truncate_mode='level', p=5)\n",
    "plt.title(\"Dendrogram\")\n",
    "plt.xlabel(\"Job Titles\")\n",
    "plt.ylabel(\"Distance\")\n",
    "plt.show()\n",
    "\n",
    "threshold = 3 \n",
    "pandas_df['cluster'] = fcluster(linkage_matrix, threshold, criterion='distance')\n",
    "\n",
    "df_with_clusters = spark.createDataFrame(pandas_df)\n",
    "print(df_with_clusters.select(\"cluster\").distinct().count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb2d1e33-d6f5-4fef-824d-b1b3f9681282",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "genai.configure(api_key=\"API key\")\n",
    "model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "cluster_descriptions = df_with_clusters.groupBy(\"cluster\").agg(\n",
    "    concat_ws(\", \", collect_list(\"cleaned_job_title\")).alias(\"job_titles\")\n",
    ")\n",
    "\n",
    "cluster_descriptions_pd = cluster_descriptions.toPandas()\n",
    "\n",
    "def generate_cluster_names(job_titles_list, delay=5):\n",
    "    cluster_names = []\n",
    "    for job_titles in job_titles_list:\n",
    "        if len(job_titles) > 1024:\n",
    "            job_titles = job_titles[:1024]\n",
    "        \n",
    "        prompt = f\"Here are job titles: {job_titles}. Provide a short and general name for this cluster:\"\n",
    "        try:\n",
    "            response = model.generate_content(prompt)\n",
    "            cluster_name = response.text.strip() if response and response.candidates else \"Unknown Cluster\"\n",
    "        except Exception as e:\n",
    "            cluster_name = f\"Error: {str(e)}\"\n",
    "        cluster_names.append(cluster_name)\n",
    "        \n",
    "        time.sleep(delay)\n",
    "    return cluster_names\n",
    "cluster_descriptions_pd[\"cluster_name\"] = generate_cluster_names(cluster_descriptions_pd[\"job_titles\"].tolist())\n",
    "\n",
    "cluster_descriptions_with_names = spark.createDataFrame(cluster_descriptions_pd)\n",
    "\n",
    "cluster_descriptions_with_names.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45a18399-4135-4871-99b0-69ad27d0723f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pd_df = pd.read_csv(\"/PATH/cluster_with_names.csv\")\n",
    "cluster_descriptions_with_names = spark.createDataFrame(pd_df)\n",
    "df_with_clusters_named = cluster_descriptions_with_names.join(df_with_clusters, on=\"cluster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55ac4835-4a5d-43e7-9307-125e387989bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pd_df = pd.read_csv(\"/PATH/JOINED_DATA.csv\")\n",
    "df_with_clusters_named = spark.createDataFrame(pd_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6783b91b-415c-4a81-a6e9-615740e287c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**How Good is our Cluster?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97c83852-8203-40d0-882a-9c79fc9370c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"cleaned_job_title\", StringType(), True),\n",
    "    StructField(\"cluster\", IntegerType(), True),\n",
    "    StructField(\"embeddings\", ArrayType(FloatType()), True)\n",
    "])\n",
    "\n",
    "parsed_rdd = df_with_clusters_named.rdd.map(\n",
    "    lambda row: (\n",
    "        row['cleaned_job_title'], \n",
    "        row['cluster'], \n",
    "        np.fromstring(row['embeddings'].strip(\"[]\"), sep=\",\").tolist()\n",
    "    )\n",
    ")\n",
    "parsed_df = spark.createDataFrame(parsed_rdd, schema)\n",
    "\n",
    "features = np.array(parsed_df.select(\"embeddings\").rdd.map(lambda row: row[0]).collect())\n",
    "clusters = np.array(parsed_df.select(\"cluster\").rdd.map(lambda row: row[0]).collect())\n",
    "\n",
    "features = np.vstack(features)\n",
    "\n",
    "calinski_harabasz = calinski_harabasz_score(features, clusters)\n",
    "print(\"Calinski-Harabasz Index:\", calinski_harabasz)\n",
    "distances = pairwise_distances(features)\n",
    "intra_cluster_distances = []\n",
    "\n",
    "\n",
    "for cluster in np.unique(clusters):\n",
    "    cluster_indices = np.where(clusters == cluster)[0]\n",
    "    cluster_points = distances[np.ix_(cluster_indices, cluster_indices)]\n",
    "    \n",
    "    avg_distance = cluster_points.sum() / (len(cluster_indices) ** 2)\n",
    "    intra_cluster_distances.append(avg_distance)\n",
    "\n",
    "intra_cluster_cohesion = np.mean(intra_cluster_distances)\n",
    "print(\"Intra-Cluster Cohesion (Overall Average Within-Cluster Distance):\", intra_cluster_cohesion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f245ac4d-d729-4027-b223-566e6ff4fa40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Extracting Skills from Description**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c40a6d28-c866-4c12-a3bd-6e75b176ecc3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_with_clusters_named = df_with_clusters_named.withColumn(\"Description\", regexp_replace(col(\"Description\"), r\"[\\n\\r\\t]\", \"  \"))\n",
    "df_with_clusters_named = df_with_clusters_named.withColumn(\"description_lower \", lower(col(\"Description\")))\n",
    "df_with_clusters_named = df_with_clusters_named.withColumn(\"Description_cleaned\", regexp_replace(col(\"description_lower \"), \"[^a-zA-Z\\s]\", \"  \"))\n",
    "\n",
    "df_with_clusters_named = df_with_clusters_named.drop(\"description_lower\")\n",
    "df_with_clusters_named.display()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"jjzha/jobbert_skill_extraction\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"jjzha/jobbert_skill_extraction\")\n",
    "def chunk_text(text, max_length=512):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    chunks = []\n",
    "    for i in range(0, len(tokens), max_length - 2):  \n",
    "        chunk = tokens[i:i + max_length - 2]\n",
    "        chunks.append(tokenizer.convert_tokens_to_string(chunk))\n",
    "    return chunks\n",
    "\n",
    "def extract_skills(description):\n",
    "    chunks = chunk_text(description)\n",
    "    skills = []\n",
    "    for chunk in chunks:\n",
    "        inputs = tokenizer(chunk, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        predicted_class_ids = torch.argmax(logits, dim=2)\n",
    "        tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "        predicted_classes = [model.config.id2label[class_id.item()] for class_id in predicted_class_ids[0]]\n",
    "\n",
    "        skill = \"\"\n",
    "        for token, predicted_class in zip(tokens, predicted_classes):\n",
    "            if \"#\" not in token:\n",
    "                if predicted_class == \"B\":\n",
    "                    if skill:\n",
    "                        skills.append(skill.strip())\n",
    "                    skill = token\n",
    "                elif predicted_class == \"I\":\n",
    "                    skill += f\" {token} \"\n",
    "        if skill:\n",
    "            skills.append(skill.strip())\n",
    "    return skills\n",
    "extract_skills_udf = udf(extract_skills, ArrayType(StringType()))\n",
    "\n",
    "result_df = df_with_clusters_named.withColumn(\"skills1\", extract_skills_udf(\"Description_cleaned\"))\n",
    "df = result_df.toPandas()\n",
    "df.to_csv(\"/PATH/extract_skills.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c93e8759-5f92-4477-af7f-f299daf39148",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pd_df = pd.read_csv(\"/PATH/extract_skills.csv\")\n",
    "df_with_clusters = spark.createDataFrame(pd_df)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"jjzha/jobbert_knowledge_extraction\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"jjzha/jobbert_knowledge_extraction\")\n",
    "def chunk_text(text, max_length=512):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    chunks = []\n",
    "    for i in range(0, len(tokens), max_length - 2):\n",
    "        chunk = tokens[i:i + max_length - 2]\n",
    "        chunks.append(tokenizer.convert_tokens_to_string(chunk))\n",
    "    return chunks\n",
    "\n",
    "def extract_skills(description):\n",
    "    chunks = chunk_text(description)\n",
    "    skills = []\n",
    "    for chunk in chunks:\n",
    "        inputs = tokenizer(chunk, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        predicted_class_ids = torch.argmax(logits, dim=2)\n",
    "        tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "        predicted_classes = [model.config.id2label[class_id.item()] for class_id in predicted_class_ids[0]]\n",
    "\n",
    "        skill = \"\"\n",
    "        for token, predicted_class in zip(tokens, predicted_classes):\n",
    "            if \"#\" not in token:\n",
    "                if predicted_class == \"B\":\n",
    "                    if skill:\n",
    "                        skills.append(skill.strip())\n",
    "                    skill = token\n",
    "                elif predicted_class == \"I\":\n",
    "                    skill += f\" {token} \"\n",
    "        if skill:\n",
    "            skills.append(skill.strip())\n",
    "    return skills\n",
    "extract_skills_udf = udf(extract_skills, ArrayType(StringType()))\n",
    "\n",
    "result_df2 = result_df.withColumn(\"skills2\", extract_skills_udf(\"Description_cleaned\"))\n",
    "df = result_df2.toPandas()\n",
    "df.to_csv(\"/PATH/extract_skills_full.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "398adc6f-e3fa-4598-bddd-c1b5c7cd59d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def combine_skills_udf(skills1, skills2):\n",
    "    try:\n",
    "        def extract_skills(skills):\n",
    "            if not skills or skills == \"[]\":\n",
    "                return []\n",
    "            skills = re.findall(r\"'(.*?)'\", skills)\n",
    "            return [skill.strip() for skill in skills]\n",
    "\n",
    "        skills1_list = extract_skills(skills1)\n",
    "        skills2_list = extract_skills(skills2)\n",
    "        combined = list(set(skills1_list + skills2_list))\n",
    "        return combined\n",
    "    except Exception as e:\n",
    "        return []\n",
    "\n",
    "combine_skills = udf(combine_skills_udf, ArrayType(StringType()))\n",
    "\n",
    "df = pd.read_csv(\"/PATH/extract_skills_full.csv\")\n",
    "spark_df = spark.createDataFrame(df)\n",
    "combined = spark_df.join(df_updated, on=\"cleaned_job_title\")\n",
    "\n",
    "df_combined = combined.withColumn(\n",
    "    \"combined_skills\",\n",
    "    combine_skills(col(\"skills1\"), col(\"skills2\"))\n",
    ")\n",
    "\n",
    "skills_exploded = df_combined.withColumn(\"skill\", explode(col(\"combined_skills\")))\n",
    "skills_count = skills_exploded.groupBy(\"Full State Name\", \"skill\").agg(count(\"skill\").alias(\"count\"))\n",
    "\n",
    "window_spec = Window.partitionBy(\"Full State Name\").orderBy(desc(\"count\"))\n",
    "ranked_skills = skills_count.withColumn(\"rank\", row_number().over(window_spec))\n",
    "top_skills_per_state = ranked_skills.filter(col(\"rank\") <= 5).orderBy(\"Full State Name\", \"rank\")\n",
    "\n",
    "state_skills_combined = top_skills_per_state.groupBy(\"Full State Name\") \\\n",
    "    .agg(collect_list(col(\"skill\")).alias(\"Top_Skills\"))\n",
    "\n",
    "state_skills_combined.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b73600d-c2e4-4ef5-a1e0-ad93102c34ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def combine_skills_udf(skills1, skills2):\n",
    "    try:\n",
    "        def extract_skills(skills):\n",
    "            if not skills or skills == \"[]\":\n",
    "                return []\n",
    "            skills = re.findall(r\"'(.*?)'\", skills)\n",
    "            return [skill.strip() for skill in skills]\n",
    "\n",
    "        skills1_list = extract_skills(skills1)\n",
    "        skills2_list = extract_skills(skills2)\n",
    "        combined = list(set(skills1_list + skills2_list))\n",
    "        return combined\n",
    "    except Exception as e:\n",
    "        return []\n",
    "\n",
    "combine_skills_udf = udf(combine_skills_udf, ArrayType(StringType()))\n",
    "spark_df = spark_df.withColumn(\n",
    "    \"combined_skills\",\n",
    "    combine_skills_udf(col(\"skills1\"), col(\"skills2\"))\n",
    ")\n",
    "df_cluster_skills = (\n",
    "    spark_df.groupBy(\"cluster\")\n",
    "    .agg(collect_list(\"combined_skills\").alias(\"skills_nested_array\"))\n",
    "    .withColumn(\"skills\", flatten(\"skills_nested_array\"))\n",
    "    .select(\"cluster\", \"skills\")\n",
    ")\n",
    "df_cluster_skills.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "76c24e53-faa6-4ae7-bb35-59e2798d0b19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Profiles Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19679ea8-0d1d-4f3d-b12d-0759b485b006",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "profiles = spark.read.parquet('/dbfs/linkedin_people_train_data')\n",
    "profiles = profiles.replace([\"N/A\", \"None\",\"null\",\"\"], None)\n",
    "selected_profiles = profiles.drop('avatar','country_code','current_company:company_id'\n",
    ",'current_company:name','educations_details', 'followers', 'following','groups', 'people_also_viewed', 'posts', 'recommendations_count', 'timestamp', 'url')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58a7d51b-c0dc-4a55-a747-55fb074f2a50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def total_degrees_fileds(education):\n",
    "    Nulls = [None,\"Null\",\"null\",\"None\",\"\",\"N/A\"]\n",
    "    if education in Nulls:\n",
    "        return None\n",
    "    degree = []\n",
    "    field = []\n",
    "    for i in education:\n",
    "        if i[\"degree\"] not in Nulls:\n",
    "            degree.append(i[\"degree\"])\n",
    "        if i[\"field\"] not in Nulls:\n",
    "            field.append(i[\"field\"])\n",
    "    return degree, field\n",
    "\n",
    "degree_udf = udf(lambda edu: total_degrees_fileds(edu)[0], ArrayType(StringType()))\n",
    "field_udf = udf(lambda edu: total_degrees_fileds(edu)[1], ArrayType(StringType()))\n",
    "selected_profiles = selected_profiles.withColumn(\"degree\", degree_udf(selected_profiles[\"education\"])).withColumn(\"field\", field_udf(selected_profiles[\"education\"]))\n",
    "def positions(experience):\n",
    "    Nulls = [None,\"Null\",\"null\",\"None\",\"\",\"N/A\"]\n",
    "    titles = []\n",
    "    if experience in Nulls:\n",
    "        return None\n",
    "    for i in experience:\n",
    "        if i[\"positions\"] in Nulls:\n",
    "            titles.append(i[\"title\"])\n",
    "        else:\n",
    "             for j in i[\"positions\"]:\n",
    "                 if j[\"title\"] not in Nulls:\n",
    "                     titles.append(j[\"title\"])\n",
    "    return titles\n",
    "\n",
    "positions_udf = udf(lambda exp: positions(exp), ArrayType(StringType()))\n",
    "selected_profiles = selected_profiles.withColumn(\"previous_positions\", positions_udf(selected_profiles[\"experience\"]))\n",
    "selected_profiles = selected_profiles.drop('experience').drop('education')\n",
    "def total_languages(languages):\n",
    "    Nulls = [None,\"Null\",\"null\",\"None\",\"\",\"N/A\"]\n",
    "    if languages in Nulls:\n",
    "        return None\n",
    "    languages = []\n",
    "    for i in languages:\n",
    "        if i[\"title\"] not in Nulls:\n",
    "            languages.append(i[\"title\"])\n",
    "    return languages\n",
    "\n",
    "language_udf = udf(lambda exp: total_languages(exp), ArrayType(StringType()))\n",
    "selected_profiles = selected_profiles.withColumn(\"total_languages\", language_udf(selected_profiles[\"languages\"]))\n",
    "def total_volunteering(volunteer_experience):\n",
    "    Nulls = [None,\"Null\",\"null\",\"None\",\"\",\"N/A\"]\n",
    "    volunteer = []\n",
    "    if volunteer_experience in Nulls:\n",
    "        return None\n",
    "    for i in volunteer_experience:\n",
    "        if i[\"title\"] not in Nulls:\n",
    "            volunteer.append(i[\"title\"])\n",
    "    return volunteer\n",
    "\n",
    "volunteer_udf = udf(lambda exp: total_volunteering(exp), ArrayType(StringType()))\n",
    "selected_profiles = selected_profiles.withColumn(\"total_volunteering\", language_udf(selected_profiles[\"volunteer_experience\"]))\n",
    "def total_courses(courses):\n",
    "    Nulls = [None,\"Null\",\"null\",\"None\",\"\",\"N/A\"]\n",
    "    total_courses= []\n",
    "    if courses in Nulls:\n",
    "        return None\n",
    "    for i in courses:\n",
    "        if i[\"title\"] not in Nulls:\n",
    "            total_courses.append(i[\"title\"])\n",
    "    return total_courses\n",
    "courses_udf = udf(lambda course: total_courses(course), ArrayType(StringType()))\n",
    "selected_profiles = selected_profiles.withColumn(\"total_courses\", courses_udf(selected_profiles[\"сourses\"]))\n",
    "\n",
    "def total_certifications(certifications):\n",
    "    Nulls = [None,\"Null\",\"null\",\"None\",\"\",\"N/A\"]\n",
    "    total_certifications= []\n",
    "    if certifications in Nulls:\n",
    "        return None\n",
    "    for i in certidications:\n",
    "        if i[\"title\"] not in Nulls:\n",
    "            total_certifications.append(i[\"title\"])\n",
    "    return total_certifications\n",
    "\n",
    "certifications_udf = udf(lambda certification: total_certifications(certification), ArrayType(StringType()))\n",
    "selected_profiles = selected_profiles.withColumn(\"total_certifications\", courses_udf(selected_profiles[\"certifications\"]))\n",
    "selected_profiles = selected_profiles.drop('languages').drop('volunteer_experience')\n",
    "selected_profiles = selected_profiles.drop('сourses')\n",
    "selected_profiles.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10ab26ec-84bd-4f89-ac66-8108546438c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "selected_profiles = selected_profiles.withColumn(\"all_experiences\",  concat(\n",
    "        col(\"previous_positions\"),  \n",
    "        array(\"position\"), \n",
    "        col(\"total_volunteering\") \n",
    "    )\n",
    ")\n",
    "selected_profiles = selected_profiles.withColumn(\"qualifications\", concat(\n",
    "    col(\"total_languages\"),\n",
    "    col(\"total_courses\"), \n",
    "    col(\"degree\"),\n",
    "    col(\"field\"),\n",
    "    col(\"total_certifications\")))\n",
    "\n",
    "selected_profiles = selected_profiles.drop('previous_positions').drop('position').drop('total_volunteering').drop('total_languages').drop('degree').drop('field').drop('total_courses').drop('total_certifications').drop('certifications')\n",
    "selected_profiles.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0658c0dd-0872-4f5d-8002-c559c9b1eb0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Matching Samples from Profiles to The Clusters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "222e16c6-efec-426d-9358-7a6a551e1dbc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sampled_df = selected_profiles.sample(withReplacement=False, fraction=0.015)\n",
    "\n",
    "apis = [\"API key\"]\n",
    "broadcast_api_key = spark.sparkContext.broadcast(apis[0])\n",
    "current_key_index = 0\n",
    "def match_clusters_with_genai(person_experience, person_qualifications, clusters):\n",
    "    prompt = f\"\"\"\n",
    "You are a clustering assistant. Match the following job experiences and qualifications to the most relevant clusters. \n",
    "If no experiences or qualifications are provided, assign the person to clusters that require no experience or qualifications.\n",
    "\n",
    "Person's job experiences: {person_experience if len(person_experience) else \"No experience provided\"}\n",
    "Person's qualifications: {person_qualifications if len(person_qualifications) else \"No qualifications provided\"}\n",
    "Available clusters and their jobs: {clusters}\n",
    "\n",
    "Respond with a comma-separated list of cluster numbers. If no exact match is found, include the most relevant clusters.\n",
    "\"\"\"\n",
    "\n",
    "    global current_key_index\n",
    "    genai.configure(api_key=apis[current_key_index])  \n",
    "    model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "    try:\n",
    "        response = model.generate_content(prompt)\n",
    "        sleep(2)\n",
    "        matches = response.text.strip() if response and response.candidates else \"\"\n",
    "        time.sleep(8)\n",
    "        return matches.split(\",\")\n",
    "    except Exception as e:\n",
    "        error_message = str(e)\n",
    "        if \"exhausted\" in error_message or \"request limit per minute\" in error_message:\n",
    "                current_key_index = (current_key_index + 1) % len(apis)\n",
    "                print(f\"Switching to the next API key: {apis[current_key_index % len(apis)]}\")\n",
    "        else:\n",
    "            return [f\"Error during API call: {e}\"]\n",
    "        \n",
    "\n",
    "match_udf = udf(\n",
    "    lambda experience, qualifications, cluster_jobs: match_clusters_with_genai(experience, qualifications, cluster_jobs),\n",
    "    ArrayType(StringType())\n",
    ")\n",
    "\n",
    "clusters_df = df_with_clusters_named.select(\"cluster\", \"job_titles\")\n",
    "clusters_dict = clusters_df.rdd.map(lambda row: (row[\"cluster\"], row[\"job_titles\"])).collectAsMap()\n",
    "clusters_str = \", \".join([f\"Cluster {k}: {v}\" for k, v in clusters_dict.items()])\n",
    "\n",
    "result_df = sampled_df.withColumn(\n",
    "    \"matched_clusters\",\n",
    "    match_udf(col(\"all_experiences\"), col(\"qualifications\"), lit(clusters_str)) \n",
    ")\n",
    "df = result_df.toPandas()\n",
    "df.to_csv(\"/PATH/profiles_with_clusters_sample_6.csv\")# change the number each time you run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5f821193-773e-41ec-b40a-cbadbf37e180",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Adding Skills for Each Samples Profiles**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff0f7287-3cac-4dc8-ae6f-7b3abdc95ab5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"/PATH/profiles_with_clusters_sample_1.csv\")#change the number to the sample you want to work with\n",
    "df1 = spark.createDataFrame(df1)\n",
    "df1.display()\n",
    "def extract_clusters_udf(clusters):\n",
    "    try:\n",
    "        if not clusters or clusters == \"[]\":  \n",
    "            return []\n",
    "        clusters_list = re.findall(r\"'(.*?)'\", clusters)\n",
    "        return [cluster.strip() for cluster in clusters_list]  \n",
    "    except Exception as e:\n",
    "        return []\n",
    "\n",
    "extract_clusters = udf(extract_clusters_udf, ArrayType(StringType()))\n",
    "df_extracted_clusters = df1.withColumn(\"extracted_clusters\", extract_clusters(col(\"matched_clusters\")))\n",
    "\n",
    "df_extracted_clusters.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef73e380-46e6-4708-bb76-f3bdc8694cc1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "skills_exploded = df_cluster_skills.withColumn(\"skills_exploded\", explode(col(\"skills\")))\n",
    "skills_count = skills_exploded.groupBy(\"cluster\", \"skills_exploded\").agg(count(\"skills_exploded\").alias(\"count\"))\n",
    "\n",
    "window_spec = Window.partitionBy(\"cluster\").orderBy(desc(\"count\"))\n",
    "ranked_skills = skills_count.withColumn(\"rank\", row_number().over(window_spec))\n",
    "top_skills_per_cluster = ranked_skills.filter(col(\"rank\") <= 5).orderBy(\"cluster\", \"rank\")\n",
    "\n",
    "\n",
    "top_cluster_skills = top_skills_per_cluster.groupBy(\"cluster\") \\\n",
    "    .agg(collect_list(col(\"skills_exploded\")).alias(\"Top_Skills\"))\n",
    "\n",
    "top_cluster_skills.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebeef42e-54b0-427a-9a60-ac7e9411d707",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_exploded = df_extracted_clusters.withColumn(\"cluster\", explode(col(\"extracted_clusters\")))\n",
    "df_joined = df_exploded.join(top_cluster_skills, on=\"cluster\", how=\"left\")\n",
    "\n",
    "df_skills = df_joined.groupBy(\"id\").agg(collect_list(\"Top_Skills\").alias(\"skills\"))\n",
    "df_result = df_extracted_clusters.join(df_skills, on=\"id\", how=\"left\")\n",
    "\n",
    "used_profiles = df_result.select(\"id\", \"about\", \"city\", \"name\", \"all_experiences\", \"qualifications\", \"skills\")\n",
    "used_profiles.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e632d391-c67b-4934-89b5-85c8ef688760",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def city_to_state(city):\n",
    "    if not city:\n",
    "        return \"United States\"\n",
    "    states = state_dict.values()\n",
    "    for state in states:\n",
    "        if state in city:\n",
    "            return state\n",
    "        \n",
    "city_to_state_udf = udf(city_to_state, StringType())\n",
    "used_profiles = used_profiles.withColumn(\"state\", city_to_state_udf(col(\"city\")))\n",
    "used_profiles = used_profiles.drop(\"city\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e496c4d2-e641-4b71-897b-a92bc8530079",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def extract_list_udf(input_str):\n",
    "    try:\n",
    "        if not input_str or input_str == \"[]\":  \n",
    "            return []\n",
    "        # Extract items inside single quotes\n",
    "        extracted_list = re.findall(r\"'(.*?)'\", input_str)\n",
    "        return [item.strip() for item in extracted_list]\n",
    "    except Exception as e:\n",
    "        return []\n",
    "\n",
    "# Register the UDF\n",
    "extract_list = udf(extract_list_udf, ArrayType(StringType()))\n",
    "used_profiles = (\n",
    "    used_profiles.withColumn(\"qualifications_list\", extract_list(col(\"qualifications\")))\n",
    "       .withColumn(\"experience_list\", extract_list(col(\"all_experiences\")))\n",
    ")\n",
    "used_profiles = used_profiles.withColumn(\"flattened_skills\", array_distinct(flatten(col(\"skills\"))))\n",
    "used_profiles = used_profiles.drop(\"skills\")\n",
    "used_profiles = used_profiles.withColumnRenamed(\"flattened_skills\", \"skills\")\n",
    "used_profiles.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a691c177-066e-40bd-918d-4e029d7295e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "\n",
    "             \n",
    "bert_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "\n",
    "def get_average_embedding(text_list):\n",
    "    if not text_list:\n",
    "        return np.zeros(bert_model.get_sentence_embedding_dimension())  \n",
    "    embeddings = bert_model.encode(text_list, convert_to_tensor=False)\n",
    "    return np.mean(embeddings, axis=0)  \n",
    "\n",
    "\n",
    "def calculate_similarity(experience, qualification, added_skills):\n",
    "\n",
    "       \n",
    "        experience = experience if isinstance(experience, list) else []\n",
    "        qualification = qualification if isinstance(qualification, list) else []\n",
    "        added_skills = added_skills if isinstance(added_skills, list) else []\n",
    "  \n",
    "        if not experience and not qualification or not added_skills:\n",
    "            return 0.0\n",
    "   \n",
    "        experience_embedding = get_average_embedding(experience)\n",
    "        qualification_embedding = get_average_embedding(qualification)\n",
    "        skill_embeddings = bert_model.encode(added_skills, convert_to_tensor=False)\n",
    "        similarities1 = cosine_similarity([experience_embedding], skill_embeddings)[0]\n",
    "        similarities2 = cosine_similarity([qualification_embedding], skill_embeddings)[0]\n",
    "\n",
    "        if  float(np.mean(similarities1)) >  float(np.mean(similarities2)):\n",
    "            return float(np.mean(similarities1))\n",
    "        else:\n",
    "            return float(np.mean(similarities2))\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "similarity_udf = udf(calculate_similarity, DoubleType())\n",
    "\n",
    "\n",
    "profiles_with_skills = used_profiles.filter(\n",
    "    col(\"skills\").isNotNull()&\n",
    "    col(\"qualifications\").isNotNull() & \n",
    "    col(\"all_experiences\").isNotNull()&\n",
    "   ( col(\"qualifications\") != \"[]\") &\n",
    "   (col(\"all_experiences\") != \"[]\") &\n",
    "   (col(\"all_experiences\") != \"[--]\"))\n",
    "\n",
    "result_df = profiles_with_skills.withColumn(\n",
    "    \"similarity_score\", \n",
    "    similarity_udf(\n",
    "        profiles_with_skills[\"experience_list\"], \n",
    "        profiles_with_skills[\"qualifications_list\"], \n",
    "        profiles_with_skills[\"skills\"]\n",
    "    )\n",
    ")\n",
    "\n",
    "average_similarity = result_df.agg(mean(\"similarity_score\").alias(\"avg_similarity_score\")).collect()[0][\"avg_similarity_score\"]\n",
    "\n",
    "\n",
    "result_df.display()  \n",
    "print(f\"Average Similarity Score: {average_similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d73fdfb6-ede3-482a-82cd-790504df2969",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Courses Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89bfacef-0574-4646-b19d-a4a8380ad053",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "coursera_courses = pd.read_csv(\"/PATH/coursera_course_dataset.csv\")\n",
    "coursera_courses = spark.createDataFrame(coursera_courses)\n",
    "coursera_courses.display()\n",
    "edx_courses = pd.read_csv(\"/PATH/data/edx.csv\")\n",
    "edx_courses = spark.createDataFrame(edx_courses)\n",
    "edx_courses.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8667c1b-6f3f-448a-9659-32c379a77766",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def convert_to_list(associatedskills):\n",
    "    try:\n",
    "        if associatedskills:\n",
    "            return  associatedskills.split(\",\") \n",
    "        return None \n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "convert_to_list_udf = udf(convert_to_list, ArrayType(StringType()))\n",
    "edx_courses = edx_courses.withColumn(\"associatedskills_list\", convert_to_list_udf(col(\"associatedskills\")))\n",
    "\n",
    "edx_courses = edx_courses.drop(\"associatedskills\")\n",
    "edx_courses.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "641c13fc-5829-4e5c-9b79-105569a3ca04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Recommending Courses Based on Top Skills in Each State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3deeec8c-0be6-4a28-aded-0606eaad70ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "029d8ad6-5081-4d68-b6da-fcfa88c02fbb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "profiles_without_skills = used_profiles.filter(col(\"skills\").isNull())\n",
    "profiles_without_skills_sampled = profiles_without_skills.limit(200)\n",
    "profiles_without_skills_sampled.display()\n",
    "import spacy\n",
    "from pyspark.sql.functions import udf, col, broadcast, collect_list, explode\n",
    "from pyspark.sql.types import ArrayType, FloatType, StringType\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "def compute_vectors(skill_list):\n",
    "    return [nlp(skill).vector.tolist() for skill in skill_list] if skill_list else []\n",
    "\n",
    "compute_vectors_udf = udf(compute_vectors, ArrayType(ArrayType(FloatType())))\n",
    "\n",
    "state_skills_combined = state_skills_combined.withColumn(\n",
    "    \"skill_vectors\", compute_vectors_udf(col(\"Top_Skills\"))\n",
    ")\n",
    "edx_courses = edx_courses.withColumn(\n",
    "    \"course_skill_vectors\", compute_vectors_udf(col(\"associatedskills_list\"))\n",
    ")\n",
    "\n",
    "\n",
    "state_skills_combined_broadcast = broadcast(state_skills_combined)\n",
    "edx_courses_broadcast = broadcast(edx_courses)\n",
    "\n",
    "\n",
    "def find_similar_skills(skill_vectors, course_skill_vectors, threshold=0.8):\n",
    "    if not skill_vectors or not course_skill_vectors:\n",
    "        return []\n",
    "    matched_skills = []\n",
    "    for skill_vec in skill_vectors:\n",
    "        for course_skill_vec in course_skill_vectors:\n",
    "            similarity = cosine_similarity([skill_vec], [course_skill_vec])[0][0]\n",
    "            if similarity >= threshold:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "find_similar_skills_udf = udf(find_similar_skills, BooleanType())\n",
    "\n",
    "\n",
    "recommendations = (\n",
    "    profiles_without_skills\n",
    "    .join(state_skills_combined_broadcast, col(\"Full State Name\") == col(\"state\"))\n",
    "    .crossJoin(edx_courses_broadcast)\n",
    "    .withColumn(\n",
    "        \"matched_skills\",\n",
    "        find_similar_skills_udf(col(\"skill_vectors\"), col(\"course_skill_vectors\"))\n",
    "    )\n",
    "    .filter(col(\"matched_skills\")==True)\n",
    ")\n",
    "\n",
    "\n",
    "recommendations_grouped = recommendations.groupBy(\"id\").agg(\n",
    "    collect_list(\"title\").alias(\"recommended_courses\")\n",
    ")\n",
    "df = recommendations_grouped.toPandas()\n",
    "df.to_csv(\"/PATH/recommendations_grouped_edx.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ab9d776-35a6-401e-872e-68fd6827b067",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def convert_to_list(associatedskills):\n",
    "    try:\n",
    "        if associatedskills:\n",
    "            return  associatedskills.split(\",\") \n",
    "        return None \n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "convert_to_list_udf = udf(convert_to_list, ArrayType(StringType()))\n",
    "coursera_courses = coursera_courses.withColumn(\"associatedskills_list\", convert_to_list_udf(col(\"Skills\")))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40bfc18a-3341-4500-8444-2a5a473eaa96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from pyspark.sql.functions import udf, col, broadcast, collect_list, explode\n",
    "from pyspark.sql.types import ArrayType, FloatType, StringType\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "def compute_vectors(skill_list):\n",
    "    return [nlp(skill).vector.tolist() for skill in skill_list] if skill_list else []\n",
    "\n",
    "compute_vectors_udf = udf(compute_vectors, ArrayType(ArrayType(FloatType())))\n",
    "\n",
    "state_skills_combined = state_skills_combined.withColumn(\n",
    "    \"skill_vectors\", compute_vectors_udf(col(\"Top_Skills\"))\n",
    ")\n",
    "coursera_courses = coursera_courses.withColumn(\n",
    "    \"course_skill_vectors\", compute_vectors_udf(col(\"associatedskills_list\"))\n",
    ")\n",
    "\n",
    "state_skills_combined_broadcast = broadcast(state_skills_combined)\n",
    "coursera_courses_broadcast = broadcast(coursera_courses)\n",
    "\n",
    "\n",
    "def find_similar_skills(skill_vectors, course_skill_vectors, threshold=0.8):\n",
    "    if not skill_vectors or not course_skill_vectors:\n",
    "        return []\n",
    "    matched_skills = []\n",
    "    for skill_vec in skill_vectors:\n",
    "        for course_skill_vec in course_skill_vectors:\n",
    "            similarity = cosine_similarity([skill_vec], [course_skill_vec])[0][0]\n",
    "            if similarity >= threshold:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "find_similar_skills_udf = udf(find_similar_skills, BooleanType())\n",
    "\n",
    "\n",
    "recommendations = (\n",
    "    profiles_without_skills\n",
    "    .join(state_skills_combined_broadcast, col(\"Full State Name\") == col(\"state\"))\n",
    "    .crossJoin(coursera_courses_broadcast)\n",
    "    .withColumn(\n",
    "        \"matched_skills\",\n",
    "        find_similar_skills_udf(col(\"skill_vectors\"), col(\"course_skill_vectors\"))\n",
    "    )\n",
    "    .filter(col(\"matched_skills\")==True)\n",
    ")\n",
    "\n",
    "\n",
    "recommendations_grouped = recommendations.groupBy(\"id\").agg(\n",
    "    collect_list(\"title\").alias(\"recommended_courses\")\n",
    ")\n",
    "df = recommendations_grouped.toPandas()\n",
    "df.to_csv(\"/PATH/recommendations_grouped_coursea.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "project",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
